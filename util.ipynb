{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/sco/thumb/d/d1/University_College_London_logo.svg/1280px-University_College_London_logo.svg.png\" width=\"400px\" align = \"left\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for Detection of Parkinson Disease Severity with Motion Sensor Arrays\n",
    "## Utility Functions For Frequency Domain Data Analysis\n",
    "`\n",
    "Last Modified: 30th Jul 2020\n",
    "Author: Ken Yew Piong\n",
    "Department: MEng Electronic Engineering with Computer Science\n",
    "`\n",
    "### Description:\n",
    "#### This is helper notebook containing all the pre-requisite functions for main.ipynb to work with following features: \n",
    "```tex\n",
    "1. Data Pre-processing: High Pass Filter to remove gravity component DC offset of accelerometer sensor data\n",
    "2. Data Pre-processing: Fast Fourier Transform to transform time series sensor data into discrete frequency components\n",
    "3. Feature Engineering: Extraction of insightful frequency domain features of PD gestures using statistical tools (e.g.: mean, std, iqr, skewness, kurtosis) \n",
    "4. Data Visualisation: Visualisation of frequency domain features against different levels of UPDRS rating PD severity\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, import_ipynb, mpld3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from scipy.stats import iqr, skew, kurtosis, variation, pearsonr\n",
    "from scipy.fftpack import fft, fftfreq\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.0 Data Pre-processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_HPF(df, highcut, fs, order):\n",
    "    \"\"\"\n",
    "    :FUNCTION: Processes raw dataframes extracted from CSV files, applies a high pass filter with\n",
    "    the specified high cut-off frequency and returns the dataframe with new appended columns \n",
    "    consisting of processed high-passed values for each x,y,z axes data\n",
    "    :df: Pandas dataframe of raw acc or gyro data (data after using pd.read_csv)\n",
    "    :highcut: Desired higher cut-off frequency \n",
    "    :fs: Sampling frequency (50 Hz for MBient sensors)\n",
    "    :order: Polynomial order of Butterworth filter\n",
    "    :file: CSV filename being worked on\n",
    "    :rtype: Processed Pandas dataframe with new filtered x,y,z values\n",
    "    \"\"\"\n",
    "    processed_df = df.copy() # copies the dataframe leaving the original df intact\n",
    "    select_cols = processed_df.columns[-3:] # list of the last three column names of the input df to enumerate later\n",
    "    col_name = ['filtered x-axis (g)', 'filtered y-axis (g)', 'filtered z-axis (g)'] # names of new processed columns\n",
    "    \n",
    "    # Butterworth filter for High-Pass applications\n",
    "    nyq = 0.5 * fs\n",
    "    high = highcut / nyq\n",
    "    b, a = signal.butter(order, high, btype='high', analog=False)\n",
    "    \n",
    "    # Apply high-pass filter and store processed data into new appended columns\n",
    "    for idx, col in enumerate(select_cols):\n",
    "        processed_df[col_name[idx]] = pd.DataFrame(signal.lfilter(b, a, processed_df.loc[:, col])).values\n",
    "        \n",
    "    return processed_df # returns a processed df with new appended columns consisting of processed x,y,z values\n",
    "\n",
    "def process_FFT(df, fs):\n",
    "    \"\"\"\n",
    "    :FUNCTION: Processes raw dataframes extracted from CSV files, applies a Fast Fourier Transform\n",
    "    and returns the dataframe with new appended columns consisting of processed FFT values for each x,y,z axes data\n",
    "    :df: Pandas dataframe of raw acc or gyro data (data after using pd.read_csv)\n",
    "    \n",
    "    :fs: Sampling frequency (50 Hz for MBient sensors)\n",
    "    :rtype: Processed Pandas dataframe with new FFT x,y,z values\n",
    "    \"\"\"\n",
    "    processed_df = df.copy() # copies the dataframe leaving the original df intact\n",
    "    select_cols = processed_df.columns[-3:] # list of the last three column names of the input df to enumerate later\n",
    "    col_name = ['FFT magnitude x-axis', 'FFT magnitude y-axis', 'FFT magnitude z-axis'] # names of new processed columns\n",
    "    result_df = pd.DataFrame() # instantiate dataframe\n",
    "    \n",
    "    # Instantiate matrix for FFT\n",
    "    lgth, num_signal=processed_df.shape\n",
    "    fqy = np.zeros([lgth, num_signal])\n",
    "\n",
    "    # Perform FFT and store processed data into new appended columns\n",
    "    for idx, col in enumerate(select_cols): \n",
    "        result_df['frequency (Hz)'] = np.arange(int(lgth/2))/(int(lgth/2)/(fs/2))\n",
    "        fqy[:,idx] = np.abs(fft(processed_df.loc[:, col].tolist()))\n",
    "        result_df[col_name[idx]] = pd.Series(fqy[0:int(lgth/2),idx])\n",
    "\n",
    "    return result_df # returns a processed df with new appended columns consisting of processed x,y,z values\n",
    "\n",
    "def process_PSD(df, fs):\n",
    "    \"\"\"\n",
    "    :DEPENDANCIES: Requires output from def process_FFT(df, fs)\n",
    "    :FUNCTION: Processes FFT dataframes, calculates the corresponding Power Spectral Density (PSD) values\n",
    "    and returns the dataframe with new appended columns consisting of processed PSD values for each x,y,z axes data\n",
    "    :df: Pandas dataframe of raw acc or gyro data (data after usi\n",
    "    ng pd.read_csv)\n",
    "    :fs: Sampling frequency (50 Hz for MBient sensors)\n",
    "    :rtype: Processed Pandas dataframe with new FFT x,y,z values\n",
    "    \"\"\"\n",
    "    processed_df = df.copy() # copies the dataframe leaving the original df intact\n",
    "    select_cols = processed_df.columns[-3:] # list of the last three column names of the input df to enumerate later\n",
    "    psd_col_name = ['psd x-axis', 'psd y-axis', 'psd z-axis'] # names of new processed columns\n",
    "    result_df = pd.DataFrame() # instantiate dataframe\n",
    "\n",
    "    # Perform PSD calculations and store processed data into new appended columns\n",
    "    for idx, col in enumerate(select_cols):\n",
    "        f_data, psd_data = signal.welch(processed_df.loc[:, col], fs)\n",
    "        result_df['frequency (Hz)'] = pd.Series(f_data)\n",
    "        result_df[psd_col_name[idx]] = pd.Series(psd_data)\n",
    "\n",
    "    return result_df # returns a processed df with new appended columns consisting of processed x,y,z values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.0 Data Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(data_name, plot_df, savefig):\n",
    "    \"\"\"\n",
    "    :FUNCTION: Dynamically plots any type of dataframe (pre-processed or not) and applies the appropriate axes customisation\n",
    "    :data_name: Name of the dataframe (e.g.: ftap-lvl0-accel-hpf-fft) which can be obtained from the keys of the database dictionary containing all dataframes\n",
    "    :plot_df: Any Pandas dataframe\n",
    "    :savefig: Boolean to toggle whether to save the figure in .png or not\n",
    "    :rtype: void - plots the graph of the input dataframe\n",
    "    \"\"\"\n",
    "    # =============================================\n",
    "    # Data extraction for plot\n",
    "    # =============================================\n",
    "    # Horizontal axis values required for the plot will always be 'frequency (Hz)'\n",
    "    x_data = 'frequency (Hz)'\n",
    "    \n",
    "    # Pinpointing the required columns to extract the vertical axis values required for the plot\n",
    "    if 'fft' in data_name:\n",
    "        col_name = ['FFT magnitude x-axis', 'FFT magnitude y-axis', 'FFT magnitude z-axis']\n",
    "    elif 'psd' in data_name: \n",
    "        col_name = ['psd x-axis', 'psd y-axis', 'psd z-axis']\n",
    "    else: \n",
    "        col_name = plot_df.columns[-3:]\n",
    "        x_data = 'elapsed (s)'\n",
    "\n",
    "    # =============================================\n",
    "    # Plot triaxial data\n",
    "    # =============================================\n",
    "    fig, ax = plt.subplots() # Instantiate object tuples for MatplotLib plots\n",
    "    color_map = ['r', 'g', 'b'] # List of color codes to enumerate to when plotting\n",
    "    labels = ['x-component', 'y-component', 'z-component'] # List of legend labels to enumerate to when plotting\n",
    "    \n",
    "    # Plot all three x, y and z axes in the same graph\n",
    "    for i in range(3):\n",
    "        ax.plot(plot_df.loc[:, x_data], plot_df.loc[:, col_name[i]], color_map[i], label=labels[i])\n",
    "\n",
    "    # =============================================\n",
    "    # Plot Axes Customisation\n",
    "    # =============================================\n",
    "    # x-label names depending on datatype \n",
    "    if 'fft' in data_name or 'psd' in data_name:\n",
    "        ax.set_xlabel('Frequency (Hz)')\n",
    "    else: \n",
    "        ax.set_xlabel('Time (s)')\n",
    "\n",
    "    # y-label names depending on datatype\n",
    "    if 'psd' in data_name: \n",
    "        ax.set_ylabel('Power Spectral Density (g^2/Hz)')\n",
    "    elif 'accel' in data_name: \n",
    "        ax.set_ylabel('Magnitude (g)')\n",
    "    elif 'gyro' in data_name:\n",
    "        ax.set_ylabel('Magnitude (deg/s)')\n",
    "\n",
    "    # Further plot customisations\n",
    "    ax.set_title(data_name)\n",
    "    ax.legend(loc='best')\n",
    "    ax.grid()\n",
    "\n",
    "    # =============================================\n",
    "    # Save Plot\n",
    "    # =============================================\n",
    "    # Toggle to save the figure as a .png file\n",
    "    if savefig == True: \n",
    "        fig.savefig(f'{data_name}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.0 Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_summary_stats(df, stats_type, lst):\n",
    "    \"\"\"\n",
    "    :DEPENDANCIES: Requires output from any pre-processing functions (e.g.: process_FFT, process_PSD)\n",
    "    :FUNCTION: Processes the pre-processed dataframes, extract statistical feature values\n",
    "    and returns a new dataframe with columns consisting of feature values for each x,y,z axes data\n",
    "    :df: Pandas dataframe of raw acc or gyro data (data after using pd.read_csv)\n",
    "    :stats_type: String input of type of statistics to extract (e.g.: ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max'])\n",
    "    :lst: List of dataframes of all severity level belonging to the same gesture and sensor type\n",
    "    e.g.: ftap_accel_fft_lst = ['ftap-lvl0-accel-hpf-fft', 'ftap-lvl1-accel-hpf-fft','ftap-lvl2-accel-hpf-fft', 'ftap-lvl3-accel-hpf-fft', 'ftap-lvl4-accel-hpf-fft']\n",
    "    :rtype: Processed Pandas dataframe with new x,y,z feature values\n",
    "    \"\"\"\n",
    "    stats = [0, 0, 0]\n",
    "    index_lst = [-3, -2, -1]\n",
    "    stats_df = {'data': [], 'stats_type': stats_type, 'x': [], 'y': [], 'z': [], 'r': []} # Gather all information in a dictionary\n",
    "    for key in lst:\n",
    "        processed_df = df[key] # running for loop iterations for each dataframe in 'lst' (e.g.: ['ftap-lvl0-accel-hpf-fft', 'ftap-lvl1-accel-hpf-fft','ftap-lvl2-accel-hpf-fft', 'ftap-lvl3-accel-hpf-fft', 'ftap-lvl4-accel-hpf-fft'])\n",
    "        # Create columns containing the computed feature values and append to a new dataframe, stats_df\n",
    "        stats_df['data'].append(key) # list of worked dataframes\n",
    "        for idx, val in enumerate(index_lst):\n",
    "            stats[idx] = processed_df.describe().loc[stats_type, processed_df.columns[val]]\n",
    "        resultant = np.sqrt(stats[0]**2 + stats[1]**2 + stats[2]**2)\n",
    "        stats_df['x'].append(stats[0])\n",
    "        stats_df['y'].append(stats[1])\n",
    "        stats_df['z'].append(stats[2])\n",
    "        stats_df['r'].append(resultant)\n",
    "    return stats_df\n",
    "\n",
    "\n",
    "def process_IQR_stats(df, lst):\n",
    "    stats = [0, 0, 0]\n",
    "    index_lst = [-3, -2, -1]\n",
    "    stats_df = {'data': [], 'stats_type': 'iqr', 'x': [], 'y': [], 'z': [], 'r': []} # Gather all information in a dictionary\n",
    "    for key in lst:\n",
    "        processed_df = df[key] # running for loop iterations for each dataframe in 'lst' (e.g.: ['ftap-lvl0-accel-hpf-fft', 'ftap-lvl1-accel-hpf-fft','ftap-lvl2-accel-hpf-fft', 'ftap-lvl3-accel-hpf-fft', 'ftap-lvl4-accel-hpf-fft'])\n",
    "        # Create columns containing the computed feature values and append to a new dataframe, stats_df\n",
    "        stats_df['data'].append(key) # list of worked dataframes\n",
    "        for idx, val in enumerate(index_lst):\n",
    "            stats[idx] = iqr(processed_df.loc[:, processed_df.columns[val]])\n",
    "        resultant = np.sqrt(stats[0]**2 + stats[1]**2 + stats[2]**2)\n",
    "        stats_df['x'].append(stats[0])\n",
    "        stats_df['y'].append(stats[1])\n",
    "        stats_df['z'].append(stats[2])\n",
    "        stats_df['r'].append(resultant)\n",
    "    return stats_df\n",
    "\n",
    "def process_variation_stats(df, lst):\n",
    "    stats = [0, 0, 0]\n",
    "    index_lst = [-3, -2, -1]\n",
    "    stats_df = {'data': [], 'stats_type': 'cov', 'x': [], 'y': [], 'z': [], 'r': []} # Gather all information in a dictionary\n",
    "    for key in lst:\n",
    "        processed_df = df[key] # running for loop iterations for each dataframe in 'lst' (e.g.: ['ftap-lvl0-accel-hpf-fft', 'ftap-lvl1-accel-hpf-fft','ftap-lvl2-accel-hpf-fft', 'ftap-lvl3-accel-hpf-fft', 'ftap-lvl4-accel-hpf-fft'])\n",
    "        # Create columns containing the computed feature values and append to a new dataframe, stats_df\n",
    "        stats_df['data'].append(key) # list of worked dataframes\n",
    "        for idx, val in enumerate(index_lst):\n",
    "            stats[idx] = variation(processed_df.loc[:, processed_df.columns[val]])\n",
    "        resultant = np.sqrt(stats[0]**2 + stats[1]**2 + stats[2]**2)\n",
    "        stats_df['x'].append(stats[0])\n",
    "        stats_df['y'].append(stats[1])\n",
    "        stats_df['z'].append(stats[2])\n",
    "        stats_df['r'].append(resultant)\n",
    "    return stats_df\n",
    "\n",
    "def process_skew_stats(df, lst):\n",
    "    stats = [0, 0, 0]\n",
    "    index_lst = [-3, -2, -1]\n",
    "    stats_df = {'data': [], 'stats_type': 'skewness', 'x': [], 'y': [], 'z': [], 'r': []} # Gather all information in a dictionary\n",
    "    for key in lst:\n",
    "        processed_df = df[key] # running for loop iterations for each dataframe in 'lst' (e.g.: ['ftap-lvl0-accel-hpf-fft', 'ftap-lvl1-accel-hpf-fft','ftap-lvl2-accel-hpf-fft', 'ftap-lvl3-accel-hpf-fft', 'ftap-lvl4-accel-hpf-fft'])\n",
    "        # Create columns containing the computed feature values and append to a new dataframe, stats_df\n",
    "        stats_df['data'].append(key) # list of worked dataframes\n",
    "        for idx, val in enumerate(index_lst):\n",
    "            stats[idx] = skew(processed_df.loc[:, processed_df.columns[val]])\n",
    "        resultant = np.sqrt(stats[0]**2 + stats[1]**2 + stats[2]**2)\n",
    "        stats_df['x'].append(stats[0])\n",
    "        stats_df['y'].append(stats[1])\n",
    "        stats_df['z'].append(stats[2])\n",
    "        stats_df['r'].append(resultant)\n",
    "    return stats_df\n",
    "\n",
    "def process_kurt_stats(df, lst):\n",
    "    stats = [0, 0, 0]\n",
    "    index_lst = [-3, -2, -1]\n",
    "    stats_df = {'data': [], 'stats_type': 'kurtosis', 'x': [], 'y': [], 'z': [], 'r': []} # Gather all information in a dictionary\n",
    "    for key in lst:\n",
    "        processed_df = df[key] # running for loop iterations for each dataframe in 'lst' (e.g.: ['ftap-lvl0-accel-hpf-fft', 'ftap-lvl1-accel-hpf-fft','ftap-lvl2-accel-hpf-fft', 'ftap-lvl3-accel-hpf-fft', 'ftap-lvl4-accel-hpf-fft'])\n",
    "        # Create columns containing the computed feature values and append to a new dataframe, stats_df\n",
    "        stats_df['data'].append(key) # list of worked dataframes\n",
    "        for idx, val in enumerate(index_lst):\n",
    "            stats[idx] = kurtosis(processed_df.loc[:, processed_df.columns[val]])\n",
    "        resultant = np.sqrt(stats[0]**2 + stats[1]**2 + stats[2]**2)\n",
    "        stats_df['x'].append(stats[0])\n",
    "        stats_df['y'].append(stats[1])\n",
    "        stats_df['z'].append(stats[2])\n",
    "        stats_df['r'].append(resultant)\n",
    "    return stats_df\n",
    "\n",
    "def process_pearsonr_stats(df, lst):\n",
    "    stats_df = {'data': [], 'stats_type': 'correlation', 'x-y': [], 'y-z': [], 'x-z': []} # Gather all information in a dictionary\n",
    "    for key in lst:\n",
    "        processed_df = df[key] # running for loop iterations for each dataframe in 'lst' (e.g.: ['ftap-lvl0-accel-hpf-fft', 'ftap-lvl1-accel-hpf-fft','ftap-lvl2-accel-hpf-fft', 'ftap-lvl3-accel-hpf-fft', 'ftap-lvl4-accel-hpf-fft'])\n",
    "        # Create columns containing the computed feature values and append to a new dataframe, stats_df\n",
    "        stats_df['data'].append(key) # list of worked dataframes\n",
    "        stats_df['x-y'].append(pearsonr(processed_df.loc[:, processed_df.columns[-3]], processed_df.loc[:, processed_df.columns[-2]]))\n",
    "        stats_df['y-z'].append(pearsonr(processed_df.loc[:, processed_df.columns[-2]], processed_df.loc[:, processed_df.columns[-1]]))\n",
    "        stats_df['x-z'].append(pearsonr(processed_df.loc[:, processed_df.columns[-3]], processed_df.loc[:, processed_df.columns[-1]]))\n",
    "    return stats_df\n",
    "\n",
    "def process_averaged_stats(stats_df_1, stats_df_2, stats_df_3):\n",
    "    stats_df = {'data': '', 'stats_type': stats_df_1['stats_type'], 'x_mean': [], 'y_mean': [], 'z_mean': [], 'r_mean': [], 'x_std': [], 'y_std': [], 'z_std': [], 'r_std': []} # Gather all information in a dictionary\n",
    "    data_name = stats_df_1['data'][0].split('-')\n",
    "    data_name.pop(1)\n",
    "    data_name.pop(3)\n",
    "    joined_name = '-'.join(data_name)\n",
    "    stats_df['data'] = joined_name\n",
    "\n",
    "    for i in range(5):\n",
    "        x_mean = np.mean([list(stats_df_1.values())[2][i], list(stats_df_2.values())[2][i], list(stats_df_3.values())[2][i]])\n",
    "        y_mean = np.mean([list(stats_df_1.values())[3][i], list(stats_df_2.values())[3][i], list(stats_df_3.values())[3][i]])\n",
    "        z_mean = np.mean([list(stats_df_1.values())[4][i], list(stats_df_2.values())[4][i], list(stats_df_3.values())[4][i]])\n",
    "        r_mean = np.mean([list(stats_df_1.values())[5][i], list(stats_df_2.values())[5][i], list(stats_df_3.values())[5][i]])\n",
    "        \n",
    "        x_std = np.std([list(stats_df_1.values())[2][i], list(stats_df_2.values())[2][i], list(stats_df_3.values())[2][i]])\n",
    "        y_std = np.std([list(stats_df_1.values())[3][i], list(stats_df_2.values())[3][i], list(stats_df_3.values())[3][i]])\n",
    "        z_std = np.std([list(stats_df_1.values())[4][i], list(stats_df_2.values())[4][i], list(stats_df_3.values())[4][i]])\n",
    "        r_std = np.std([list(stats_df_1.values())[5][i], list(stats_df_2.values())[5][i], list(stats_df_3.values())[5][i]])\n",
    "        \n",
    "        stats_df['x_mean'].append(x_mean)\n",
    "        stats_df['y_mean'].append(y_mean)\n",
    "        stats_df['z_mean'].append(z_mean)\n",
    "        stats_df['r_mean'].append(r_mean)\n",
    "        \n",
    "        stats_df['x_std'].append(x_std)\n",
    "        stats_df['y_std'].append(y_std)\n",
    "        stats_df['z_std'].append(z_std)\n",
    "        stats_df['r_std'].append(r_std)\n",
    "    return stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.0 Statistical Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stats(stats_df, savefig):\n",
    "    \"\"\"\n",
    "    :DEPENDANCIES: Requires the output of def process_stats(df, stats_type, lst)\n",
    "    :FUNCTION: Plots the statistical trends of stats_df by all 5 level of severity\n",
    "    :stats_df: Dataframe containing the feature extracted statistics output from def plot_stats\n",
    "    :savefig: Boolean to toggle whether to save the figure in .png or not\n",
    "    :rtype: void - plots the statistical trends graph by all 5 level of severity\n",
    "    \"\"\"\n",
    "    # =============================================\n",
    "    # Plot statistical data\n",
    "    # =============================================\n",
    "    fig, ax = plt.subplots() # Instantiate object tuples for MatplotLib plots\n",
    "    color_map = ['r', 'g', 'b'] # List of color codes to enumerate to when plotting\n",
    "    labels = ['x-component', 'y-component', 'z-component'] # List of legend labels to enumerate to when plotting\n",
    "\n",
    "     # Plot all 5 levels of severity in the same graph\n",
    "    for idx, key in enumerate(list(stats_df.keys())[2:]):\n",
    "        ax.plot(stats_df[key], '-o', linewidth=2, markersize=10, color=color_map[idx], label=labels[idx])\n",
    "        for i in range(5):\n",
    "            ax.text(i, stats_df[key][i], f'({i}, {round(stats_df[key][i], 3)})', fontsize=18) # add coordinate labels for each severity level\n",
    "\n",
    "    # =============================================\n",
    "    # Plot Axes Customisation\n",
    "    # =============================================\n",
    "    # x-label\n",
    "    ax.set_xlabel('Level of Parkinson Disease Severity')\n",
    "\n",
    "    # y-label names depending on datatype\n",
    "    if 'psd' in stats_df['data'][0]:\n",
    "        ax.set_ylabel('Power Spectral Density (V^2/Hz)')\n",
    "    elif 'accel' in stats_df['data'][0]:\n",
    "        ax.set_ylabel('Amplitude of Frequency Domain (g)')\n",
    "    elif 'gyro' in stats_df['data'][0]:\n",
    "        ax.set_ylabel('Amplitude of Frequency Domain (deg/s)')\n",
    "\n",
    "    # Further plot customisations\n",
    "    title = stats_df['data'][0]\n",
    "    edit = title.split('-')\n",
    "    edit.pop(1)\n",
    "    title = '-'.join(edit)\n",
    "    ax.set_xticks([0, 1, 2, 3, 4])\n",
    "    ax.set_title(f\"{title}-{stats_df['stats_type']}\")\n",
    "    ax.legend(loc='best')\n",
    "    ax.grid()\n",
    "\n",
    "    # =============================================\n",
    "    # Save Plot\n",
    "    # =============================================\n",
    "    # Toggle to save the figure as a .png file\n",
    "    if savefig == True: \n",
    "        fig.savefig(f\"{title}-{stats_df['stats_type']}.png\")\n",
    "        \n",
    "def plot_stats_with_error_bars(stats_df, savefig, show_coordinates):\n",
    "    \"\"\"\n",
    "    :DEPENDANCIES: Requires the output of def process_stats(df, stats_type, lst)\n",
    "    :FUNCTION: Plots the statistical trends of stats_df by all 5 level of severity\n",
    "    :stats_df: Dataframe containing the feature extracted statistics output from def plot_stats\n",
    "    :savefig: Boolean to toggle whether to save the figure in .png or not\n",
    "    :rtype: void - plots the statistical trends graph by all 5 level of severity\n",
    "    \"\"\"\n",
    "    # =============================================\n",
    "    # Plot statistical data\n",
    "    # =============================================\n",
    "    fig, ax = plt.subplots() # Instantiate object tuples for MatplotLib plots\n",
    "    color_map = ['r', 'g', 'b', 'm'] # List of color codes to enumerate to when plotting\n",
    "    labels = ['x-component', 'y-component', 'z-component', 'resultant'] # List of legend labels to enumerate to when plotting\n",
    "\n",
    "    for idx, mean, std, color_map, labels in zip([[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4]], list(stats_df.values())[2:6], list(stats_df.values())[6:10], color_map, labels):\n",
    "        ax.errorbar(idx, mean, yerr = std, fmt='-o', linewidth=2, capsize=10, markeredgewidth=2, markersize=10, elinewidth=2, color=color_map, label=labels)\n",
    "        if show_coordinates == True:\n",
    "            for i, val in zip(idx, mean):\n",
    "                ax.text(i, val, f'({i}, {round(val, 3)})', fontsize=18) # add coordinate labels for each severity level\n",
    "\n",
    "    # =============================================\n",
    "    # Plot Axes Customisation\n",
    "    # =============================================\n",
    "    # x-label\n",
    "    ax.set_xlabel('Level of Parkinson Disease Severity')\n",
    "\n",
    "    # y-label names depending on datatype\n",
    "    if 'time' in stats_df['data']:\n",
    "        if 'accel' in stats_df['data']:\n",
    "            ax.set_ylabel('Amplitude of Time Domain (g)')\n",
    "        elif 'gyro' in stats_df['data']:\n",
    "            ax.set_ylabel('Amplitude of Time Domain (deg/s)')\n",
    "    elif 'psd' in stats_df['data']:\n",
    "        ax.set_ylabel('Power Spectral Density (g^2/Hz)')\n",
    "    elif 'accel' in stats_df['data']:\n",
    "        ax.set_ylabel('Amplitude of Frequency Domain (g)')\n",
    "    elif 'gyro' in stats_df['data']:\n",
    "        ax.set_ylabel('Amplitude of Frequency Domain (deg/s)')\n",
    "\n",
    "    # Further plot customisations\n",
    "    title = stats_df['data']\n",
    "    ax.set_xticks([0, 1, 2, 3, 4])\n",
    "    ax.set_title(f\"{title}-{stats_df['stats_type']}\")\n",
    "    ax.legend(loc='best')\n",
    "    ax.grid()\n",
    "\n",
    "    # =============================================\n",
    "    # Save Plot\n",
    "    # =============================================\n",
    "    # Toggle to save the figure as a .png file\n",
    "    if savefig == True: \n",
    "        fig.savefig(f\"{title}-{stats_df['stats_type']}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.0 Web Application Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_metrics(df, filename):\n",
    "    if 'accel' in filename:\n",
    "        x = 'x-axis (g)'\n",
    "        y = 'y-axis (g)'\n",
    "        z = 'z-axis (g)'\n",
    "        r = 'resultant (g)'\n",
    "        \n",
    "    elif 'gyro' in filename:\n",
    "        x = 'x-axis (deg/s)'\n",
    "        y = 'y-axis (deg/s)'\n",
    "        z = 'z-axis (deg/s)'\n",
    "        r = 'resultant (deg/s)'\n",
    "        \n",
    "    stats = [0, 0, 0]\n",
    "    index_lst = [-3, -2, -1]\n",
    "    stats_dict = {'metric name': [], 'metric description': [], x: [], y: [], z: [], r: []}\n",
    "    \n",
    "    if 'time' in filename:\n",
    "        keyword = 'time'\n",
    "    elif 'fft' in filename: \n",
    "        keyword = 'frequency'\n",
    "    elif 'psd' in filename:\n",
    "        keyword = 'power spectral density'\n",
    "        \n",
    "    # Mean\n",
    "    stats_dict['metric name'].append('mean')\n",
    "    stats_dict['metric description'].append(f'Computed average value of amplitudes in the {keyword} domain')\n",
    "    for idx, val in enumerate(index_lst):\n",
    "        stats[idx] = df.describe().loc['mean', df.columns[val]]\n",
    "    resultant = np.sqrt(stats[0]**2 + stats[1]**2 + stats[2]**2)\n",
    "    stats_dict[x].append(stats[0])\n",
    "    stats_dict[y].append(stats[1])\n",
    "    stats_dict[z].append(stats[2])\n",
    "    stats_dict[r].append(resultant)\n",
    "\n",
    "    # Standard Deviation\n",
    "    stats_dict['metric name'].append('std')\n",
    "    stats_dict['metric description'].append(f'Computed standard deviation of amplitudes in the {keyword} domain')\n",
    "    for idx, val in enumerate(index_lst):\n",
    "        stats[idx] = df.describe().loc['std', df.columns[val]]\n",
    "    resultant = np.sqrt(stats[0]**2 + stats[1]**2 + stats[2]**2)\n",
    "    stats_dict[x].append(stats[0])\n",
    "    stats_dict[y].append(stats[1])\n",
    "    stats_dict[z].append(stats[2])\n",
    "    stats_dict[r].append(resultant)\n",
    "\n",
    "    # Median\n",
    "    stats_dict['metric name'].append('median')\n",
    "    stats_dict['metric description'].append(f'50th percentile value of the amplitudes in the {keyword} domain')\n",
    "    for idx, val in enumerate(index_lst):\n",
    "        stats[idx] = df.describe().loc['50%', df.columns[val]]\n",
    "    resultant = np.sqrt(stats[0]**2 + stats[1]**2 + stats[2]**2)\n",
    "    stats_dict[x].append(stats[0])\n",
    "    stats_dict[y].append(stats[1])\n",
    "    stats_dict[z].append(stats[2])\n",
    "    stats_dict[r].append(resultant)\n",
    "\n",
    "    # Interquartile Range\n",
    "    stats_dict['metric name'].append('iqr')\n",
    "    stats_dict['metric description'].append(f'Interquartile range of the amplitudes in the {keyword} domain')\n",
    "    for idx, val in enumerate(index_lst):\n",
    "        stats[idx] = iqr(df.loc[:, df.columns[val]])\n",
    "    resultant = np.sqrt(stats[0]**2 + stats[1]**2 + stats[2]**2)\n",
    "    stats_dict[x].append(stats[0])\n",
    "    stats_dict[y].append(stats[1])\n",
    "    stats_dict[z].append(stats[2])\n",
    "    stats_dict[r].append(resultant)\n",
    "    \n",
    "    # Minimum\n",
    "    stats_dict['metric name'].append('min')\n",
    "    stats_dict['metric description'].append(f'Absolute minimum value of amplitude in the {keyword} domain')\n",
    "    for idx, val in enumerate(index_lst):\n",
    "        stats[idx] = df.describe().loc['min', df.columns[val]]\n",
    "    resultant = np.sqrt(stats[0]**2 + stats[1]**2 + stats[2]**2)\n",
    "    stats_dict[x].append(stats[0])\n",
    "    stats_dict[y].append(stats[1])\n",
    "    stats_dict[z].append(stats[2])\n",
    "    stats_dict[r].append(resultant)\n",
    "    \n",
    "    # Maximum\n",
    "    stats_dict['metric name'].append('max')\n",
    "    stats_dict['metric description'].append(f'Absolute maximum value of amplitude in the {keyword} domain')\n",
    "    for idx, val in enumerate(index_lst):\n",
    "        stats[idx] = df.describe().loc['max', df.columns[val]]\n",
    "    resultant = np.sqrt(stats[0]**2 + stats[1]**2 + stats[2]**2)\n",
    "    stats_dict[x].append(stats[0])\n",
    "    stats_dict[y].append(stats[1])\n",
    "    stats_dict[z].append(stats[2])\n",
    "    stats_dict[r].append(resultant)\n",
    "\n",
    "    # Coefficient of Variation\n",
    "    stats_dict['metric name'].append('cov')\n",
    "    stats_dict['metric description'].append(f'Coefficient of variation of amplitudes in the {keyword} domain')\n",
    "    for idx, val in enumerate(index_lst):\n",
    "        stats[idx] = variation(df.loc[:, df.columns[val]])\n",
    "    resultant = np.sqrt(stats[0]**2 + stats[1]**2 + stats[2]**2)\n",
    "    stats_dict[x].append(stats[0])\n",
    "    stats_dict[y].append(stats[1])\n",
    "    stats_dict[z].append(stats[2])\n",
    "    stats_dict[r].append(resultant)\n",
    "\n",
    "    # Skewness of Amplitude Distribution\n",
    "    stats_dict['metric name'].append('skewness')\n",
    "    stats_dict['metric description'].append(f'Sample skewness of the amplitude distribution in the {keyword} domain')\n",
    "    for idx, val in enumerate(index_lst):\n",
    "        stats[idx] = skew(df.loc[:, df.columns[val]])\n",
    "    resultant = np.sqrt(stats[0]**2 + stats[1]**2 + stats[2]**2)\n",
    "    stats_dict[x].append(stats[0])\n",
    "    stats_dict[y].append(stats[1])\n",
    "    stats_dict[z].append(stats[2])\n",
    "    stats_dict[r].append(resultant)\n",
    "    \n",
    "    # Kurtosis of Amplitude Distribution\n",
    "    stats_dict['metric name'].append('kurtosis')\n",
    "    stats_dict['metric description'].append(f'Sample kurtosis of the amplitude distribution in the {keyword} domain')\n",
    "    for idx, val in enumerate(index_lst):\n",
    "        stats[idx] = kurtosis(df.loc[:, df.columns[val]])\n",
    "    resultant = np.sqrt(stats[0]**2 + stats[1]**2 + stats[2]**2)\n",
    "    stats_dict[x].append(stats[0])\n",
    "    stats_dict[y].append(stats[1])\n",
    "    stats_dict[z].append(stats[2])\n",
    "    stats_dict[r].append(resultant)   \n",
    "    \n",
    "    # Process Dataframe\n",
    "    stats_df = pd.DataFrame(stats_dict, columns=list(stats_dict.keys()))\n",
    "    \n",
    "    return stats_df   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
